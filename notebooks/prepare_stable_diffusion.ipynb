{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07d83fbe",
   "metadata": {},
   "source": [
    "# Подготовка "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8550ed90",
   "metadata": {},
   "source": [
    "Мы используем пример от [Lambda Labs](https://lambdalabs.com/blog/how-to-fine-tune-stable-diffusion-how-we-made-the-text-to-pokemon-model-at-lambda)\n",
    "\n",
    "\n",
    "Этот пример используется для обучения [данный репозиторий](https://github.com/justinpinkney/stable-diffusion/tree/fc3a4fd8d5e171627ef30769146c68b9c072dffb)\n",
    "\n",
    "Отредактируем yaml файл с конфигурацией обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f0148b9-f587-4728-8e48-5650d49c43d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'stable-diffusion'...\n",
      "remote: Enumerating objects: 1631, done.\u001b[K\n",
      "remote: Counting objects: 100% (721/721), done.\u001b[K\n",
      "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
      "remote: Total 1631 (delta 684), reused 663 (delta 663), pack-reused 910\u001b[K\n",
      "Receiving objects: 100% (1631/1631), 73.86 MiB | 2.14 MiB/s, done.\n",
      "Resolving deltas: 100% (1017/1017), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/justinpinkney/stable-diffusion.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eea1544",
   "metadata": {},
   "source": [
    "```yaml\n",
    "data:\n",
    "  target: main.DataModuleFromConfig\n",
    "  params:\n",
    "    batch_size: 4\n",
    "    num_workers: 4\n",
    "    num_val_workers: 0 # Avoid a weird val dataloader issue\n",
    "    train:\n",
    "      target: ldm.data.simple.hf_dataset_load_disk\n",
    "      params:\n",
    "        name: promt_dataset\n",
    "        image_transforms:\n",
    "        - target: torchvision.transforms.Resize\n",
    "          params:\n",
    "            size: 512\n",
    "            interpolation: 3\n",
    "        - target: torchvision.transforms.RandomCrop\n",
    "          params:\n",
    "            size: 512\n",
    "        - target: torchvision.transforms.RandomHorizontalFlip\n",
    "    validation:\n",
    "      target: ldm.data.simple.TextOnly\n",
    "      params:\n",
    "        captions:\n",
    "        - \"fistula on a metal pipe\"\n",
    "        - \"Photo of a fistula on a metal pipe\"\n",
    "        - \"crack on a metal pipe\"\n",
    "        - \"Photo of a crack on a metal pipe\"\n",
    "        output_size: 512\n",
    "        n_gpus: 1 # small hack to make sure we see all our samples\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c90438",
   "metadata": {},
   "source": [
    "Мы поменяли две строчки \n",
    "\n",
    "`ldm.data.simple.hf_dataset -> ldm.data.simple.hf_dataset_load_disk`\n",
    "\n",
    "`lambdalabs/pokemon-blip-captions -> promt_dataset`\n",
    "\n",
    "\n",
    "`ldm.data.simple.hf_dataset` это функция отвечающая за чтение датасета, оригинале она находиться по [этой ссылке](https://github.com/justinpinkney/stable-diffusion/blob/fc3a4fd8d5e171627ef30769146c68b9c072dffb/ldm/data/simple.py#L123)\n",
    "\n",
    "\n",
    "Нам надо создать рядом с этой же функцией, нашу функцию которая будет грузить датасет с диска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75be5e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_from_disk\n",
    "...\n",
    "def hf_dataset_load_disk(\n",
    "    name,\n",
    "    image_transforms=[],\n",
    "    image_column=\"image\",\n",
    "    text_column=\"text\",\n",
    "    split='train',\n",
    "    image_key='image',\n",
    "    caption_key='txt',\n",
    "    ):\n",
    "    \"\"\"Make huggingface dataset with appropriate list of transforms applied\n",
    "    \"\"\"\n",
    "    ds = load_from_disk(name)\n",
    "    image_transforms = [instantiate_from_config(tt) for tt in image_transforms]\n",
    "    image_transforms.extend([transforms.ToTensor(),\n",
    "                                transforms.Lambda(lambda x: rearrange(x * 2. - 1., 'c h w -> h w c'))])\n",
    "    tform = transforms.Compose(image_transforms)\n",
    "\n",
    "    assert image_column in ds.column_names, f\"Didn't find column {image_column} in {ds.column_names}\"\n",
    "    assert text_column in ds.column_names, f\"Didn't find column {text_column} in {ds.column_names}\"\n",
    "\n",
    "    def pre_process(examples):\n",
    "        processed = {}\n",
    "        processed[image_key] = [tform(im) for im in examples[image_column]]\n",
    "        processed[caption_key] = examples[text_column]\n",
    "        return processed\n",
    "\n",
    "    ds.set_transform(pre_process)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e170c1",
   "metadata": {},
   "source": [
    "Да, изначально в репозитории присудствуеи CocoImagesAndCaptionsTrain2017, но он для работы с сегментацией, к нашему варианту это не подходит "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d0726ec-2199-4d32-8c05-256147ec97cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/notebooks/stable-diffusion\n"
     ]
    }
   ],
   "source": [
    "%cd stable-diffusion/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5d5a307-058f-413e-94f4-04b1caf08581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/notebooks/libs\n",
      "/app/notebooks/libs/taming-transformers\n",
      "fatal: destination path 'taming-transformers' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "%cd libs\n",
    "%cd taming-transformers\n",
    "!git clone https://github.com/CompVis/taming-transformers.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "674a9107-a73d-4ef6-a5d0-b7a1ba287e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/app/notebooks/libs/taming-transformers/taming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0babd02a-c8d9-436a-a625-b59d5c6f9baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/notebooks/libs/taming-transformers'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48c496f5-8314-4645-b487-4eeab72cd473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd ../../\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02ec0e27-7706-4c66-85f4-c16388aa1a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import taming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0270154-b602-4111-8bf4-64a9faaa3074",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "Moving 0 files to the new cache system\n",
      "0it [00:00, ?it/s]\n",
      "Global seed set to 23\n",
      "Running on GPUs 0\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "Keeping EMAs of 688.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"stable-diffusion/main.py\", line 670, in <module>\n",
      "    model = instantiate_from_config(config.model)\n",
      "  File \"/app/notebooks/stable-diffusion/ldm/util.py\", line 79, in instantiate_from_config\n",
      "    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n",
      "  File \"/app/notebooks/stable-diffusion/ldm/models/diffusion/ddpm.py\", line 526, in __init__\n",
      "    self.instantiate_cond_stage(cond_stage_config)\n",
      "  File \"/app/notebooks/stable-diffusion/ldm/models/diffusion/ddpm.py\", line 584, in instantiate_cond_stage\n",
      "    model = instantiate_from_config(config)\n",
      "  File \"/app/notebooks/stable-diffusion/ldm/util.py\", line 79, in instantiate_from_config\n",
      "    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n",
      "  File \"/app/notebooks/stable-diffusion/ldm/modules/encoders/modules.py\", line 193, in __init__\n",
      "    self.transformer = CLIPTextModel.from_pretrained(version)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/transformers/modeling_utils.py\", line 2110, in from_pretrained\n",
      "    state_dict = load_state_dict(resolved_archive_file)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/transformers/modeling_utils.py\", line 371, in load_state_dict\n",
      "    return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/serialization.py\", line 712, in load\n",
      "    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/serialization.py\", line 1049, in _load\n",
      "    result = unpickler.load()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/serialization.py\", line 1019, in persistent_load\n",
      "    load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/serialization.py\", line 997, in load_tensor\n",
      "    storage = zip_file.get_storage_from_record(name, numel, torch._UntypedStorage).storage()._untyped()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"stable-diffusion/main.py\", line 935, in <module>\n",
      "    if trainer.global_rank == 0:\n",
      "NameError: name 'trainer' is not defined\n"
     ]
    }
   ],
   "source": [
    "!python stable-diffusion/main.py \\\n",
    "    -t \\\n",
    "    --base /app/notebooks/stable-diffusion/configs/stable-diffusion/pokemon.yaml \\\n",
    "    --gpus 0 \\\n",
    "    --scale_lr False \\\n",
    "    --num_nodes 1 \\\n",
    "    --check_val_every_n_epoch 10 \\\n",
    "    --finetune_from /app/checkpoints/sd-v1-4-full-ema.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "137ac184-4189-47ef-8518-efe31bf880c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from taming.modules.vqvae.quantize import VectorQuantizer2 as VectorQuantizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
